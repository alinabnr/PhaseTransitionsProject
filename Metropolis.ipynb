{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "58587c81",
   "metadata": {},
   "source": [
    "# Metropolis algorithm"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c04ce6dc",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "344577e3",
   "metadata": {},
   "source": [
    "## What is autocorrelation time?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c1f74a9",
   "metadata": {},
   "source": [
    "Autocorrelation time quantifies how long a system remembers its past — in other words, how many steps it takes for correlations between successive states to decay. In computational physics and statistical mechanics, this concept is essential for understanding how efficiently an algorithm samples the system’s equilibrium distribution.\n",
    "\n",
    "There are two main ways to estimate the autocorrelation time:\n",
    "\n",
    "1. Theoretically (heuristically) — by analyzing the physical processes or algorithmic mechanisms that govern relaxation (e.g., diffusion, collective motion, or energy barriers). This helps identify what slows down convergence but may miss hidden slow modes, leading to an underestimate.\n",
    "\n",
    "2. Empirically — by computing the autocorrelation function of several physical observables (like energy, magnetization, or particle density). If none of these observables couple strongly to the slowest mode, the true autocorrelation time may again be underestimated. Therefore: there\n",
    "is always the danger that our **chosen set of observables has failed to include one that has strong enough overlap with the slowest mode leading to an underestimate**.\n",
    "\n",
    "The integrated autocorrelation time, often denoted $\\tau_{\\text{int}}$, determines the effective number of independent samples collected. Physically, it corresponds to how fast the system relaxes toward equilibrium — thus linking the algorithm’s sampling efficiency to actual dynamical or collective phenomena in the model. For example, near a critical point, long-range correlations in physical quantities manifest as very long autocorrelation times, a phenomenon known as **critical slowing down**.\n",
    "\n",
    "The actual rate of convergence to equilibrium from a given initial distribution may be much faster than the worst-case estimate given by $\\tau_{\\text{exp}}$. Thus, it is usual to determine empirically when \"equilibrium\" has been achieved, by plotting selected observables as a function of time and noting when the initial transient appears to end.\n",
    "\n",
    "Watch out for **metastability**!\n",
    "For example, near a first-order phase transition, most Monte Carlo methods suffer from metastability associated with transitions between configurations typical of the distinct pure phases. We can try initial conditions typical of each of these phases (e.g., for many models, a \"hot\" start (random initial configuration) and a \"cold\" start (ordered initial start; all spins up or all spins down)). Consistency between these runs does not guarantee\n",
    "that metastability is absent, but it does give increased confidence. Plots of observables as a function of time are also useful indicators of possible metastability.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2157831e",
   "metadata": {},
   "source": [
    "**Once we estimate how long it takes to reach equilibrium**, we simply **discard (“burn in”) the initial portion of the data** — up to a cutoff time $n_{\\text{disc}}$.\n",
    "\n",
    "Although this is not strictly necessary in the asymptotic limit (since systematic errors from the transient decay faster than statistical ones), in practice, keeping non-equilibrium data can introduce significant bias if the starting configuration is far from equilibrium. Discarding the transient data therefore helps ensure that only equilibrium samples contribute to averages, improving the accuracy of estimated physical quantities.\n",
    "\n",
    "Even in **finite-size lattices**, a cutoff is required. The only difference is that, since the system is finite, its equilibration and correlation times are in principle bounded. Still, one must determine an appropriate thermalization time for each lattice size (often a few times the autocorrelation time of the slowest mode) before starting data collection.\n",
    "\n",
    "For the 2D Ising model, we discard approximately $20\\tau$ of initial data to remove initialization bias and run for at least $1000\\tau$ to ensure percent-level statistical accuracy. These ratios hold in general, though the autocorrelation time $\\tau$ increases strongly near the critical temperature due to critical slowing down and depends on both lattice size and simulation algorithm."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a7108d4",
   "metadata": {},
   "source": [
    "## Comparison of algorithms\n",
    "\n",
    "The better algorithm is the one that has the smaller autocorrelation time, when time is measured in units of computer (CPU) time."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7def39fe",
   "metadata": {},
   "source": [
    "## Implementation of algorithm for 2D Ising model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4629bb28",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e2be0f67",
   "metadata": {},
   "outputs": [],
   "source": [
    "def metropolis_algorithm(initial_state, transition_function, acceptance_function, num_iterations):\n",
    "    \"\"\"\n",
    "    Perform the Metropolis algorithm for sampling from a probability distribution.\n",
    "\n",
    "    Parameters:\n",
    "    initial_state: The starting state of the system.\n",
    "    transition_function: A function that proposes a new state given the current state.\n",
    "    acceptance_function: A function that computes the acceptance probability of the proposed state.\n",
    "    num_iterations: The number of iterations to perform.\n",
    "\n",
    "    Returns:\n",
    "    A list of sampled states.\n",
    "    \"\"\"\n",
    "    current_state = initial_state\n",
    "    samples = [current_state]\n",
    "\n",
    "    for _ in range(num_iterations):\n",
    "        proposed_state = transition_function(current_state)\n",
    "        acceptance_prob = acceptance_function(current_state, proposed_state)\n",
    "\n",
    "        if random.random() < acceptance_prob:\n",
    "            current_state = proposed_state\n",
    "\n",
    "        samples.append(current_state)\n",
    "\n",
    "    return samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "eb3ce389",
   "metadata": {},
   "outputs": [],
   "source": [
    "def Hamiltonian_2D_Ising_with_external_field(state, J, h):\n",
    "    \"\"\"\n",
    "    Calculate the Hamiltonian of a 2D Ising model with an external magnetic field.\n",
    "\n",
    "    Parameters:\n",
    "    state: A 2D list representing the spin configuration (+1 or -1).\n",
    "    J: Interaction strength between neighboring spins.\n",
    "    h: External magnetic field strength.\n",
    "\n",
    "    Returns:\n",
    "    The Hamiltonian value.\n",
    "    \"\"\"\n",
    "    H = 0\n",
    "    rows = len(state)\n",
    "    cols = len(state[0])\n",
    "\n",
    "    for i in range(rows):\n",
    "        for j in range(cols):\n",
    "            # Interaction with right neighbor\n",
    "            H -= J * state[i][j] * state[i][(j + 1) % cols]\n",
    "            # Interaction with bottom neighbor\n",
    "            H -= J * state[i][j] * state[(i + 1) % rows][j]\n",
    "            # Interaction with external magnetic field\n",
    "            H -= h * state[i][j]\n",
    "\n",
    "    return H"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
